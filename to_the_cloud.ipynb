{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "State of repair:\n",
    "In my GBQ I have 85,760,124 rows, however in the exploration files I have 85,760,124 rows? The difference is 0, or 0.0% of the data.\n",
    "\n",
    "-Complete\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "import os\n",
    "import zipfile\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pandas_gbq import to_gbq\n",
    "\n",
    "# Define the dataset and table names\n",
    "dataset_id = 'wedge-project-JBangtson.the_wedge_dataset'\n",
    "\n",
    "# Initialize a BigQuery client\n",
    "client = bigquery.Client(project='wedge-project-jbangtson')\n",
    "\n",
    "\n",
    "data_directory = \"E:\\\\College\\\\Fall 2024\\\\ADA\\\\Wedge\\\\Wedge_Project\\\\data\\\\unzipped\\\\\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the files in as lazy dataframes.\n",
    "\n",
    "This script efficiently loads multiple CSV files into Polars LazyFrame objects, allowing for optimized data processing without immediately materializing the data into memory. It selectively handles CSVs based on their naming convention (specifically files marked as \"inactive\") and manages errors gracefully during the loading process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to put all csvs into there own dfs\n",
    "#https://chatgpt.com/share/66e4ad8b-ea5c-8000-9117-d884dd0bbfb3\n",
    "\n",
    "\n",
    "# Initialize an empty list to store LazyFrames\n",
    "lazy_df_list = []\n",
    "\n",
    "# Loop through files and load lazily\n",
    "for idx, file in enumerate(os.listdir(data_directory)):\n",
    "    \n",
    "\n",
    "    file_path = os.path.join(data_directory, file)\n",
    "\n",
    "\n",
    "    if len(os.listdir(data_directory)[idx].split(\"_\")) >= 4 and os.listdir(data_directory)[idx].split(\"_\")[3] == \"inactive.csv\":\n",
    "        # Use LazyFrame for efficient processing\n",
    "        lazy_df = pl.scan_csv(file_path, has_header=True, null_values=[\"\\\\N\"], ignore_errors=True,separator=\";\")\n",
    "\n",
    "    else:\n",
    "        # Use LazyFrame for efficient processing\n",
    "        lazy_df = pl.scan_csv(file_path, has_header=True, null_values=[\"\\\\N\"], ignore_errors=True)\n",
    "    \n",
    "    # Append LazyFrame to the list\n",
    "    lazy_df_list.append(lazy_df)\n",
    "\n",
    "# Example: Materialize (collect) one of the lazy DataFrames to inspect it\n",
    "df = lazy_df_list[0].collect()\n",
    "#df1 = lazy_df_list[38].collect()\n",
    "\n",
    "clean_columns = df.columns\n",
    "#print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating GBQ Schema\n",
    "\n",
    "This code defines a schema for a BigQuery table using bigquery.SchemaField objects. The schema specifies the structure of the data, including the column names, data types (e.g., FLOAT, STRING, BOOLEAN, TIMESTAMP), and whether each field is nullable. This schema can be used to load, query, and manage structured transaction data in BigQuery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wedge_schema = [\n",
    "    bigquery.SchemaField(\"datetime\", \"TIMESTAMP\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"register_no\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"emp_no\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"trans_no\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"upc\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"description\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"trans_type\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"trans_subtype\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"trans_status\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"department\", \"FLOAT\", mode=\"NULLABLE\"),#\n",
    "    bigquery.SchemaField(\"quantity\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"scale\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"cost\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"unitPrice\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"total\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"regPrice\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"altPrice\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"tax\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"taxexempt\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"foodstamp\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"wicable\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"discount\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"memDiscount\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"discountable\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"discounttype\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"voided\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"percentDiscount\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"itemQtty\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"volDiscType\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"volume\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"volSpecial\", \"FLOAT\", mode=\"NULLABLE\"),###\n",
    "    bigquery.SchemaField(\"mixMatch\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"matched\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"memType\", \"BOOLEAN\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"staff\", \"BOOLEAN\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"numflag\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"itemstatus\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"tenderstatus\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"charflag\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"varflag\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"batchHeaderID\", \"BOOLEAN\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"local\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"organic\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"display\", \"BOOLEAN\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"receipt\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"card_no\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"store\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"branch\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"match_id\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"trans_id\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Cleaning/Casting Methods\n",
    "\n",
    "### Wedge Cleaner \n",
    "The wedge_cleaner function processes a pandas DataFrame to ensure data consistency and cleanliness by applying specific type-safe casting functions to each column based on its name. It handles columns differently based on their expected data type:\n",
    "\n",
    "    Datetime Columns: Applies datetime_safe_cast for proper datetime formatting.\n",
    "    Float Columns: Applies float_safe_cast to ensure values are converted to floats.\n",
    "    String Columns: Applies string_safe_cast for consistent string formatting.\n",
    "    Boolean Columns: Applies boolean_safe_cast to correctly convert values to boolean.\n",
    "\n",
    "The function iterates over each column, performs the appropriate casting, and returns the cleaned DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def wedge_cleaner(pandas_df):\n",
    "    # Apply the function within the loop\n",
    "\n",
    "\n",
    "    for col_name in pandas_df:\n",
    "        print(f'Column: {col_name}')\n",
    "\n",
    "        # Datetime\n",
    "\n",
    "        try:\n",
    "            if col_name == \"datetime\":\n",
    "                pandas_df[col_name] = pd.to_datetime(pandas_df[col_name])\n",
    "\n",
    "        except:\n",
    "            print(f\"Error with {col_name} column with the value {pandas_df[col_name]}\") \n",
    "\n",
    "        # Float\n",
    "        if col_name in [\"register_no\", \"emp_no\", \"trans_no\", \"department\", \"quantity\", \"scale\", \"cost\", \"unitPrice\", \"total\", \"regPrice\", \"altPrice\", \"tax\", \"taxexempt\", \"foodstamp\", \"wicable\", \"discount\", \"memDiscount\", \"discountable\", \"discounttype\", \"voided\", \"percentDiscount\", \"itemQtty\", \"volDiscType\", \"volume\", \"volSpecial\", \"mixMatch\", \"matched\", \"numflag\", \"itemstatus\", \"tenderstatus\", \"varflag\", \"local\", \"organic\", \"receipt\", \"card_no\", \"store\", \"branch\", \"match_id\", \"trans_id\"]:\n",
    "            pandas_df[col_name] = pandas_df[col_name].apply(float_safe_cast)\n",
    "\n",
    "        # String\n",
    "        if col_name in [\"upc\", \"description\", \"trans_type\", \"trans_subtype\", \"trans_status\", \"charflag\"]:\n",
    "            pandas_df[col_name] = pandas_df[col_name].apply(string_safe_cast)\n",
    "\n",
    "        # Boolean\n",
    "        if col_name in [\"memType\", \"staff\", \"batchHeaderID\", \"display\"]:\n",
    "            pandas_df[col_name] = pandas_df[col_name].apply(boolean_safe_cast)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return pandas_df\n",
    "    \n",
    "    #print(col_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Date time\n",
    "\n",
    "def datetime_safe_cast(val):\n",
    "    try:\n",
    "        return datetime.strptime(val, '%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    except (ValueError, TypeError):\n",
    "        print(f\"Returning None Datetime. Cannot convert {val} to datetime\")\n",
    "        return datetime.min # or another value to handle invalid cases\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Float\n",
    "def float_safe_cast(val):\n",
    "    try:\n",
    "        return float(val) if val is not None else None\n",
    "    except ValueError:\n",
    "        print(\"Returning None Float\")\n",
    "        return None  # or another value to handle invalid cases\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# String\n",
    "def string_safe_cast(val):\n",
    "    if str(val) in \"nan\" :\n",
    "        return \"\"\n",
    "\n",
    "    try:\n",
    "        return str(val)\n",
    "    except ValueError:\n",
    "        print(\"Returning None String\")\n",
    "        return \"\"  # or another value to handle invalid cases\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boolean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Boolean\n",
    "def boolean_safe_cast(val):\n",
    "    try:\n",
    "        return bool(val)\n",
    "    except ValueError:\n",
    "        print(\"Returning None Boolean\")\n",
    "        return None  # or another value to handle invalid cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleans and Uploads to GBQ (ðŸ™)\n",
    "\n",
    "This script processes a list of Polars LazyFrame objects by converting them to pandas DataFrames, cleaning them using the wedge_cleaner function, and then uploading the cleaned DataFrames to Google BigQuery (GBQ).\n",
    "Steps:\n",
    "Iterate Over LazyFrames:\n",
    "For each LazyFrame in lazy_df_list, the script prints the file number and name.\n",
    "Collect and Clean Data:\n",
    "Convert the LazyFrame to a pandas DataFrame (df) and set column names to clean_columns.\n",
    "Use wedge_cleaner to clean the DataFrame.\n",
    "Upload to GBQ:\n",
    "Print status messages indicating the start and completion of the upload process.\n",
    "Define the project ID and destination table name for BigQuery.\n",
    "Upload the cleaned DataFrame to BigQuery using to_gbq() with the if_exists='replace' parameter to replace the table if it already exists.\n",
    "This code ensures that each file is processed, cleaned, and uploaded to a specified BigQuery table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning file number: 0\n",
      "File Name: transArchive_201001_201003.csv\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, lazy_df \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(lazy_df_list):\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCleaning file number: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFile Name: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos\u001b[38;5;241m.\u001b[39mlistdir(data_directory)[idx]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 15\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mlazy_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m     df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m clean_columns\n\u001b[0;32m     17\u001b[0m     clean_panda_df \u001b[38;5;241m=\u001b[39m df\n",
      "File \u001b[1;32mc:\\Users\\justi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\polars\\lazyframe\\frame.py:2027\u001b[0m, in \u001b[0;36mLazyFrame.collect\u001b[1;34m(self, type_coercion, predicate_pushdown, projection_pushdown, simplify_expression, slice_pushdown, comm_subplan_elim, comm_subexpr_elim, cluster_with_columns, no_optimization, streaming, engine, background, _eager, **_kwargs)\u001b[0m\n\u001b[0;32m   2025\u001b[0m \u001b[38;5;66;03m# Only for testing purposes\u001b[39;00m\n\u001b[0;32m   2026\u001b[0m callback \u001b[38;5;241m=\u001b[39m _kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost_opt_callback\u001b[39m\u001b[38;5;124m\"\u001b[39m, callback)\n\u001b[1;32m-> 2027\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrap_df(\u001b[43mldf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Date time\n",
    "#df = lazy_df_list[45].collect()\n",
    "#df.columns = clean_columns\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#pandas_df = df.to_pandas()\n",
    "\n",
    "for idx, lazy_df in enumerate(lazy_df_list):\n",
    "    print(f\"Cleaning file number: {idx}\\nFile Name: {os.listdir(data_directory)[idx]}\")\n",
    "\n",
    "    df = lazy_df.collect()\n",
    "    df.columns = clean_columns\n",
    "    clean_panda_df = df\n",
    "    \n",
    "    clean_panda_df = wedge_cleaner(clean_panda_df)\n",
    "    \n",
    "    \n",
    "\n",
    "    print(f\"Finished cleaning file number: {idx}\\nFile Name: {os.listdir(data_directory)[idx]}!\\n\\n------------------------------------\")\n",
    "\n",
    "    print(f\"Uploading file number: {idx}\\nFile Name: {os.listdir(data_directory)[idx]} to GBQ\")\n",
    "\n",
    "    # Define project_id and destination table\n",
    "    project_id = 'wedge-project-jbangtson'\n",
    "    table_name = str(os.listdir(data_directory)[idx]).split(\".\")[0]\n",
    "    destination_table = f'the_wedge_dataset.{table_name}'\n",
    "\n",
    "\n",
    "    to_gbq(clean_panda_df, destination_table, project_id=project_id, if_exists='replace')\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning file number: 37\n",
      "File Name: transArchive_201510.csv\n",
      "Column: datetime\n",
      "Column: register_no\n",
      "Column: emp_no\n",
      "Column: trans_no\n",
      "Column: upc\n",
      "Column: description\n",
      "Column: trans_type\n",
      "Column: trans_subtype\n",
      "Column: trans_status\n",
      "Column: department\n",
      "Column: quantity\n",
      "Column: Scale\n",
      "Column: cost\n",
      "Column: unitPrice\n",
      "Column: total\n",
      "Column: regPrice\n",
      "Column: altPrice\n",
      "Column: tax\n",
      "Column: taxexempt\n",
      "Column: foodstamp\n",
      "Column: wicable\n",
      "Column: discount\n",
      "Column: memDiscount\n",
      "Column: discountable\n",
      "Column: discounttype\n",
      "Column: voided\n",
      "Column: percentDiscount\n",
      "Column: ItemQtty\n",
      "Column: volDiscType\n",
      "Column: volume\n",
      "Column: VolSpecial\n",
      "Column: mixMatch\n",
      "Column: matched\n",
      "Column: memType\n",
      "Column: staff\n",
      "Column: numflag\n",
      "Column: itemstatus\n",
      "Column: tenderstatus\n",
      "Column: charflag\n",
      "Column: varflag\n",
      "Column: batchHeaderID\n",
      "Column: local\n",
      "Column: organic\n",
      "Column: display\n",
      "Column: receipt\n",
      "Column: card_no\n",
      "Column: store\n",
      "Column: branch\n",
      "Column: match_id\n",
      "Column: trans_id\n",
      "Finished cleaning file number: 37\n",
      "File Name: transArchive_201510.csv!\n",
      "\n",
      "------------------------------------\n",
      "Uploading file number: 37\n",
      "File Name: transArchive_201510.csv to GBQ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# Date time\n",
    "#df = lazy_df_list[45].collect()\n",
    "#df.columns = clean_columns\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#pandas_df = df.to_pandas()\n",
    "\n",
    "for idx, lazy_df in enumerate(lazy_df_list[37:38], start=37):\n",
    "    print(f\"Cleaning file number: {idx}\\nFile Name: {os.listdir(data_directory)[idx]}\")\n",
    "\n",
    "    df = pd.read_csv(\"data/unzipped/\" + os.listdir(data_directory)[idx], sep=\",\")\n",
    "    # df = pain_in_ass_df\n",
    "    \n",
    "    \n",
    "    clean_panda_df = wedge_cleaner(df)\n",
    "    \n",
    "    \n",
    "\n",
    "    print(f\"Finished cleaning file number: {idx}\\nFile Name: {os.listdir(data_directory)[idx]}!\\n\\n------------------------------------\")\n",
    "\n",
    "    print(f\"Uploading file number: {idx}\\nFile Name: {os.listdir(data_directory)[idx]} to GBQ\")\n",
    "\n",
    "    # Define project_id and destination table\n",
    "    project_id = 'wedge-project-jbangtson'\n",
    "    table_name = str(os.listdir(data_directory)[idx]).split(\".\")[0]\n",
    "    destination_table = f'the_wedge_dataset.{table_name}'\n",
    "\n",
    "\n",
    "    to_gbq(clean_panda_df, destination_table, project_id=project_id, if_exists='replace')\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 banana\n",
      "2 cherry\n"
     ]
    }
   ],
   "source": [
    "fruit = [\"apple\", \"banana\", \"cherry\"]\n",
    "\n",
    "for idx, x in enumerate(fruit[1:], start=1):  # Start index from 10\n",
    "    print(idx, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\justi\\AppData\\Local\\Temp\\ipykernel_9000\\2579531584.py:3: DtypeWarning: Columns (42) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  manual_troubleshooting_df = pd.read_csv(f\"data/unzipped/{table_name}.csv\", sep=\";\")\n"
     ]
    }
   ],
   "source": [
    "#open a csv file to a pandas dataframe\n",
    "table_name = \"transArchive_201201_201203_inactive\"\n",
    "manual_troubleshooting_df = pd.read_csv(f\"data/unzipped/{table_name}.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: datetime\n",
      "Column: register_no\n",
      "Column: emp_no\n",
      "Column: trans_no\n",
      "Column: upc\n",
      "Column: description\n",
      "Column: trans_type\n",
      "Column: trans_subtype\n",
      "Column: trans_status\n",
      "Column: department\n",
      "Column: quantity\n",
      "Column: Scale\n",
      "Column: cost\n",
      "Column: unitPrice\n",
      "Column: total\n",
      "Column: regPrice\n",
      "Column: altPrice\n",
      "Column: tax\n",
      "Column: taxexempt\n",
      "Column: foodstamp\n",
      "Column: wicable\n",
      "Column: discount\n",
      "Column: memDiscount\n",
      "Column: discountable\n",
      "Column: discounttype\n",
      "Column: voided\n",
      "Column: percentDiscount\n",
      "Column: ItemQtty\n",
      "Column: volDiscType\n",
      "Column: volume\n",
      "Column: VolSpecial\n",
      "Column: mixMatch\n",
      "Column: matched\n",
      "Column: memType\n",
      "Column: staff\n",
      "Column: numflag\n",
      "Column: itemstatus\n",
      "Column: tenderstatus\n",
      "Column: charflag\n",
      "Column: varflag\n",
      "Column: batchHeaderID\n",
      "Column: local\n",
      "Column: organic\n",
      "Returning None Float\n",
      "Returning None Float\n",
      "Returning None Float\n",
      "Returning None Float\n",
      "Returning None Float\n",
      "Returning None Float\n",
      "Returning None Float\n",
      "Returning None Float\n",
      "Returning None Float\n",
      "Returning None Float\n",
      "Returning None Float\n",
      "Returning None Float\n",
      "Returning None Float\n",
      "Returning None Float\n",
      "Returning None Float\n",
      "Returning None Float\n",
      "Returning None Float\n",
      "Returning None Float\n",
      "Returning None Float\n",
      "Returning None Float\n",
      "Returning None Float\n",
      "Returning None Float\n",
      "Returning None Float\n",
      "Returning None Float\n",
      "Returning None Float\n",
      "Column: display\n",
      "Column: receipt\n",
      "Column: card_no\n",
      "Column: store\n",
      "Column: branch\n",
      "Column: match_id\n",
      "Column: trans_id\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "#convert a pandas columns datatype\n",
    "manual_troubleshooting_df = wedge_cleaner(manual_troubleshooting_df)\n",
    "\n",
    "\n",
    "# Define project_id and destination table\n",
    "project_id = 'wedge-project-jbangtson'\n",
    "\n",
    "destination_table = f'the_wedge_dataset.{table_name}'\n",
    "\n",
    "\n",
    "to_gbq(manual_troubleshooting_df, destination_table, project_id=project_id, if_exists='replace')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
