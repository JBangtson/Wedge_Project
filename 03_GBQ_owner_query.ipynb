{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GBQ Queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.api_core.exceptions import GoogleAPIError\n",
    "import os\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "client = bigquery.Client(project=\"\")\n",
    "data_directory = \"E:\\\\College\\\\Fall 2024\\\\ADA\\\\Wedge\\\\Wedge_Project\\\\data\\\\unzipped\\\\\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wedge_task_two = pd.read_csv(f'E:\\College\\Fall 2024\\ADA\\Wedge\\Wedge_Project\\owner_data_folder\\_final_owner_data.txt', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code above to view all transactions for card owners: \n",
    "\n",
    "48289, 48420, 56191, 20300, 48996, 56191, and 16551.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# List of columns\n",
    "columns = ['datetime', 'register_no', 'emp_no', 'trans_no', 'upc', 'description', 'trans_type', 'trans_subtype', 'trans_status', 'department', 'quantity', 'Scale', 'cost', 'unitPrice', 'total', 'regPrice', 'altPrice', 'tax', 'taxexempt', 'foodstamp', 'wicable', 'discount', 'memDiscount', 'discountable', 'discounttype', 'voided', 'percentDiscount', 'ItemQtty', 'volDiscType', 'volume', 'VolSpecial', 'mixMatch', 'matched', 'memType', 'staff', 'numflag', 'itemstatus', 'tenderstatus', 'charflag', 'varflag', 'batchHeaderID', 'local', 'organic', 'display', 'receipt', 'card_no', 'store', 'branch', 'match_id', 'trans_id']\n",
    "\n",
    "# Create an empty Polars DataFrame with the specified columns\n",
    "gbq_query_df = pl.DataFrame({col: [] for col in columns})\n",
    "\n",
    "print(gbq_query_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run a query\n",
    "def run_query(query):\n",
    "    try:\n",
    "        query_job = client.query(query)\n",
    "        results = query_job.result()\n",
    "\n",
    "        bytes_processed = query_job.total_bytes_processed\n",
    "        mb_processed = bytes_processed / (1024 ** 2)\n",
    "        cost_per_tb = 5.0\n",
    "\n",
    "\n",
    "        tb_processed = bytes_processed / (1024 ** 4)  # Convert bytes to terabytes\n",
    "        estimated_cost = tb_processed * cost_per_tb\n",
    "\n",
    "        # Display the processed data and estimated cost\n",
    "        print(f\"Data processed: {mb_processed:.2f} MB\")\n",
    "        \n",
    "\n",
    "        print(f\"Estimated bytes processed: {bytes_processed}\")\n",
    "        print(f\"Estimated cost: ${estimated_cost:.10f}\\n\\n\")\n",
    "\n",
    "        print(f\"Estimated bytes processed against a full year of data: {bytes_processed*50}\")\n",
    "        print(f\"Estimated cost against a full year of data: ${estimated_cost*50:.20f}\")\n",
    "        print(f\"Estimated cost against a full year of data every 6 hours: ${(estimated_cost*50)*(4*365):.20f}\\n\\n---------------\")\n",
    "\n",
    "        \n",
    "\n",
    "        return results\n",
    "    except GoogleAPIError as e:\n",
    "        print(f\"Error running query: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***This next function is a modified version of the function above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run a query\n",
    "def run_query2(query):\n",
    "    try:\n",
    "        query_job = client.query(query)\n",
    "        df = query_job.to_dataframe()\n",
    "        #results = query_job.result()\n",
    "\n",
    "        bytes_processed = query_job.total_bytes_processed\n",
    "        mb_processed = bytes_processed / (1024 ** 2)\n",
    "        cost_per_tb = 5.0\n",
    "\n",
    "\n",
    "        tb_processed = bytes_processed / (1024 ** 4)  # Convert bytes to terabytes\n",
    "        estimated_cost = tb_processed * cost_per_tb\n",
    "\n",
    "        # Display the processed data and estimated cost\n",
    "        print(f\"Data processed: {mb_processed:.2f} MB\")\n",
    "        \n",
    "\n",
    "        print(f\"Estimated bytes processed: {bytes_processed}\")\n",
    "        print(f\"Estimated cost: ${estimated_cost:.10f}\\n\\n\")\n",
    "\n",
    "        print(f\"Estimated bytes processed against a full year of data: {bytes_processed*50}\")\n",
    "        print(f\"Estimated cost against a full year of data: ${estimated_cost*50:.20f}\")\n",
    "        print(f\"Estimated cost against a full year of data every 6 hours: ${(estimated_cost*50)*(4*365):.20f}\\n\\n---------------\")\n",
    "\n",
    "        \n",
    "\n",
    "        return df.astype(str)\n",
    "    except GoogleAPIError as e:\n",
    "        print(f\"Error running query: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_card_nums = []\n",
    "\n",
    "distinct_card_nums = \"\"\"\n",
    "SELECT distinct(card_no)\n",
    "FROM `the_wedge_dataset.transArchive_*`\n",
    "WHERE card_no != 3;\n",
    "\"\"\"\n",
    "\n",
    "# Run the query and display results\n",
    "results = run_query2(distinct_card_nums)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Select 20 random numbers from the all_card_nums list\n",
    "\n",
    "# results = list(results['card_no'])\n",
    "# random_card_nums = random.sample(results, 100)\n",
    "# print(random_card_nums)\n",
    "\n",
    "# Write the random_card_nums list to a tab-delimited text file\n",
    "# with open('random_card_nums.txt', 'w') as f:\n",
    "#     for num in random_card_nums:\n",
    "#         f.write(f\"{num}\\t\")\n",
    "\n",
    "# Read the random_card_nums file into a list\n",
    "with open('random_card_nums.txt', 'r') as f:\n",
    "    random_card_nums = f.read().split('\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('random_card_nums.txt', 'r') as f:\n",
    "    random_card_nums_string = f.read()\n",
    "\n",
    "\n",
    "random_card_nums_string = random_card_nums_string.replace(\"\\t\", \", \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GBQ Owner Queries\n",
    "\n",
    "This code defines a function save_owner_query_to_file() that retrieves and saves data from a Google BigQuery dataset to a CSV file. \n",
    "\n",
    "The function takes two arguments: yearOfQuery (the year to filter the data by) and table-by-table \n",
    "(a boolean flag that controls whether to process each table individually or as a group). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def save_owner_query_to_file(yearOfQuery, tableByTable=False):\n",
    "    \n",
    "  if tableByTable:\n",
    "\n",
    "    for idx, file in enumerate(os.listdir(data_directory)):\n",
    "      owner_query_df = pd.DataFrame()\n",
    "      if f\"transArchive_{yearOfQuery}\" in file:\n",
    "      \n",
    "\n",
    "        owner_gbq_query = f\"\"\"\n",
    "        SELECT\n",
    "          *,\n",
    "        SAFE_CAST(Scale AS INT64) AS IntScale\n",
    "        FROM\n",
    "          `the_wedge_dataset.transArchive_{yearOfQuery}*`\n",
    "        WHERE card_no in ({random_card_nums_string}) \n",
    "        \"\"\"\n",
    "\n",
    "        temp_df = run_query2(owner_gbq_query)\n",
    "\n",
    "        # #temp_df = pl.DataFrame(results)\n",
    "\n",
    "        # rows = [dict(row) for row in results]\n",
    "        # #columns = list(rows[0].keys()) if rows else []\n",
    "\n",
    "        # # Create Polars DataFrame from the rows\n",
    "        # temp_df = pl.DataFrame(rows)\n",
    "\n",
    "        owner_query_df = pd.concat([owner_query_df, temp_df], ignore_index=True)\n",
    "        owner_query_df.to_csv(f'E:\\College\\Fall 2024\\ADA\\Wedge\\Wedge_Project\\owner_data_folder\\owner_data_{yearOfQuery}.txt', sep='\\t', index=False)\n",
    "    \n",
    "  else:\n",
    "\n",
    "\n",
    "     for idx, file in enumerate(os.listdir(data_directory)):\n",
    "      owner_query_df = pd.DataFrame()\n",
    "      if f\"transArchive_{yearOfQuery}\" in file:\n",
    "      \n",
    "\n",
    "        owner_gbq_query = f\"\"\"\n",
    "        SELECT\n",
    "          *,\n",
    "        SAFE_CAST(Scale AS INT64) AS IntScale\n",
    "        FROM\n",
    "          `the_wedge_dataset.{file.split('.')[0]}`\n",
    "        WHERE card_no in ({random_card_nums_string}) \n",
    "        \"\"\"\n",
    "\n",
    "        temp_df = run_query2(owner_gbq_query)\n",
    "\n",
    "        # #temp_df = pl.DataFrame(results)\n",
    "\n",
    "        # rows = [dict(row) for row in results]\n",
    "        # #columns = list(rows[0].keys()) if rows else []\n",
    "\n",
    "        # # Create Polars DataFrame from the rows\n",
    "        # temp_df = pl.DataFrame(rows)\n",
    "\n",
    "        owner_query_df = pd.concat([owner_query_df, temp_df], ignore_index=True)\n",
    "        owner_query_df.to_csv(f'E:\\College\\Fall 2024\\ADA\\Wedge\\Wedge_Project\\owner_data_folder\\owner_data_{file.split('.')[0]}_version2.txt', sep='\\t', index=False)\n",
    "\n",
    "\n",
    "# if results is None or len(results) == 0:\n",
    "#     print(\"No results found or query returned None.\")\n",
    "# else:\n",
    "#     # Process the rows if results exist\n",
    "#     rows = [dict(row) for row in results]\n",
    "#     columns = list(rows[0].keys()) if rows else []\n",
    "\n",
    "#     # Create Polars DataFrame from the rows\n",
    "#     gbq_query_df = pl.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(2010, 2018):\n",
    "    try:\n",
    "        save_owner_query_to_file(x, tableByTable=False)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e} with year {x}\")\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Queries for 2015 and 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "owner_query_df = pd.DataFrame()\n",
    "random_card_nums_string = random_card_nums_string[:-2]\n",
    "fileNameForQuery = \"transArchive_\"\n",
    "\n",
    "owner_gbq_query = f\"\"\"\n",
    "        SELECT\n",
    "          *\n",
    "        FROM\n",
    "          `the_wedge_dataset.transArchive_*`\n",
    "        WHERE card_no in ({random_card_nums_string});\n",
    "        \"\"\"\n",
    "\n",
    "temp_df = run_query2(owner_gbq_query)\n",
    "\n",
    "# #temp_df = pl.DataFrame(results)\n",
    "\n",
    "# rows = [dict(row) for row in results]\n",
    "# #columns = list(rows[0].keys()) if rows else []\n",
    "\n",
    "# # Create Polars DataFrame from the rows\n",
    "# temp_df = pl.DataFrame(rows)\n",
    "\n",
    "owner_query_df = pd.concat([owner_query_df, temp_df], ignore_index=True)\n",
    "owner_query_df.to_csv(f'E:\\College\\Fall 2024\\ADA\\Wedge\\Wedge_Project\\owner_data_folder_redo\\owner_data_{fileNameForQuery}.txt', sep='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove comma from last card number\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Containing the Data into a Single Dataframe and File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['datetime', 'register_no', 'emp_no', 'trans_no', 'upc', 'description', 'trans_type', 'trans_subtype', 'trans_status', 'department', 'quantity', 'Scale', 'cost', 'unitPrice', 'total', 'regPrice', 'altPrice', 'tax', 'taxexempt', 'foodstamp', 'wicable', 'discount', 'memDiscount', 'discountable', 'discounttype', 'voided', 'percentDiscount', 'ItemQtty', 'volDiscType', 'volume', 'VolSpecial', 'mixMatch', 'matched', 'memType', 'staff', 'numflag', 'itemstatus', 'tenderstatus', 'charflag', 'varflag', 'batchHeaderID', 'local', 'organic', 'display', 'receipt', 'card_no', 'store', 'branch', 'match_id', 'trans_id', 'IntScale']\n",
    "\n",
    "# Create an empty Polars DataFrame with the specified columns\n",
    "final_owner_df = pd.DataFrame({col: [] for col in columns})\n",
    "\n",
    "\n",
    "\n",
    "for idx, file in enumerate(os.listdir('owner_data_folder_redo')):\n",
    "    \n",
    "    try:\n",
    "        temp_df = pd.read_csv(f'E:\\College\\Fall 2024\\ADA\\Wedge\\Wedge_Project\\owner_data_folder_redo\\{file}', sep='\\t')\n",
    "        \n",
    "        final_owner_df = pd.concat([final_owner_df, temp_df], axis=0)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e} with file {file}\")\n",
    "        continue\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_owner_df.drop('IntScale', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_owner_df.to_csv('E:\\College\\Fall 2024\\ADA\\Wedge\\Wedge_Project\\owner_data_folder_redo\\_final_owner_data.txt', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
